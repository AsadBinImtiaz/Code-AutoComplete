{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison & Evaluation\n\nComprehensive comparison of model variants:\n- **M0 (Baseline)**: Base model (no fine-tuning)\n- **M1 (Trained)**: Iteratively trained model from notebook 03\n\n## Evaluation Criteria\n1. Generic code generation quality\n2. CDK-specific code generation\n3. Syntax validity\n4. Keyword matching\n5. Inference latency\n6. Overall improvement metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:20:42.594233Z",
     "start_time": "2026-01-20T17:20:42.591678Z"
    }
   },
   "source": [
    "import sys\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Dict, List"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:20:43.294536Z",
     "start_time": "2026-01-20T17:20:43.292266Z"
    }
   },
   "source": [
    "# Add project root to path\nproject_root = Path.cwd().parent\nsys.path.append(str(project_root))\n\nprint(f\"Project root: {project_root}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/tgdimas1/Projs/python/code_complete\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:20:44.824587Z",
     "start_time": "2026-01-20T17:20:44.788921Z"
    }
   },
   "source": [
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "from src.config import EVAL_DIR, BASELINE_FILE, TRAINED_FILE, OUTPUT_FILE\n",
    "print(f\"\\nEvaluation directory: {EVAL_DIR}\")\n",
    "print(\"Baseline file:\", BASELINE_FILE)\n",
    "print(\"Trained file:\", TRAINED_FILE)\n",
    "print(\"Output report file:\", OUTPUT_FILE)\n",
    "\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation directory: /Users/tgdimas1/Projs/python/code_complete/evaluation\n",
      "Baseline file: /Users/tgdimas1/Projs/python/code_complete/evaluation/baseline_m0_results.json\n",
      "Trained file: /Users/tgdimas1/Projs/python/code_complete/evaluation/trained_model_results.json\n",
      "Output report file: /Users/tgdimas1/Projs/python/code_complete/evaluation/model_comparison_report.json\n",
      "Evaluation directory: /Users/tgdimas1/Projs/python/code_complete/evaluation\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Results\n\nLoad results from baseline (M0) and trained (M1) models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T17:20:51.940619Z",
     "start_time": "2026-01-20T17:20:51.863513Z"
    }
   },
   "source": [
    "# Load baseline results\nif BASELINE_FILE.exists():\n    with open(BASELINE_FILE, 'r') as f:\n        baseline_results = json.load(f)\n    print(f\"✓ Loaded baseline (M0) results\")\n    print(f\"  Model: {baseline_results['model_name']}\")\n    print(f\"  Type: {baseline_results['model_type']}\")\nelse:\n    print(f\"✗ Baseline results not found at {BASELINE_FILE}\")\n    print(\"  Run notebook 02 first!\")\n    baseline_results = None\n\n# Load trained results\nif TRAINED_FILE.exists():\n    with open(TRAINED_FILE, 'r') as f:\n        trained_results = json.load(f)\n    print(f\"\\n✓ Loaded trained (M1) results\")\n    print(f\"  Model: {trained_results['model_name']}\")\n    print(f\"  Type: {trained_results['model_type']}\")\n    if 'training_config' in trained_results:\n        cfg = trained_results['training_config']\n        print(f\"  Training: {cfg.get('num_iterations', 'N/A')} iterations\")\n        print(f\"  Best iteration: {cfg.get('best_iteration', 'N/A')}\")\nelse:\n    print(f\"\\n✗ Trained results not found at {TRAINED_FILE}\")\n    print(\"  Run notebook 03 first!\")\n    trained_results = None\n\nif not baseline_results or not trained_results:\n    raise ValueError(\"Missing evaluation results. Run notebooks 02 and 03 first.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded baseline (M0) results\n",
      "  Model: Qwen/Qwen2.5-Coder-3B\n",
      "  Type: M0_baseline\n",
      "\n",
      "✗ Trained results not found at /Users/tgdimas1/Projs/python/code_complete/evaluation/trained_model_results.json\n",
      "  Run notebook 03 first!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing evaluation results. Run notebooks 02 and 03 first.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 30\u001B[0m\n\u001B[1;32m     27\u001B[0m     trained_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m baseline_results \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m trained_results:\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing evaluation results. Run notebooks 02 and 03 first.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Missing evaluation results. Run notebooks 02 and 03 first."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Code Generation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract generic evaluation metrics\nbaseline_gen = baseline_results['generic_evaluation']['summary']\ntrained_gen = trained_results['generic_evaluation']['summary']\n\nprint(\"Generic Code Generation Comparison\")\nprint(\"=\" * 60)\nprint(f\"\\nSyntax Validity:\")\nprint(f\"  M0 (Baseline): {baseline_gen['syntax_validity_percent']:.1f}%\")\nprint(f\"  M1 (Trained):  {trained_gen['syntax_validity_percent']:.1f}%\")\nprint(f\"  Improvement:   {trained_gen['syntax_validity_percent'] - baseline_gen['syntax_validity_percent']:+.1f}%\")\n\nprint(f\"\\nKeyword Match Rate:\")\nprint(f\"  M0 (Baseline): {baseline_gen.get('keyword_match_rate', 0)*100:.1f}%\")\nprint(f\"  M1 (Trained):  {trained_gen.get('keyword_match_rate', 0)*100:.1f}%\")\nprint(f\"  Improvement:   {(trained_gen.get('keyword_match_rate', 0) - baseline_gen.get('keyword_match_rate', 0))*100:+.1f}%\")\n\nprint(f\"\\nMean Latency:\")\nprint(f\"  M0 (Baseline): {baseline_gen['mean_latency_ms']:.1f}ms\")\nprint(f\"  M1 (Trained):  {trained_gen['mean_latency_ms']:.1f}ms\")\nprint(f\"  Change:        {trained_gen['mean_latency_ms'] - baseline_gen['mean_latency_ms']:+.1f}ms\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDK-Specific Code Generation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract CDK evaluation metrics\nbaseline_cdk = baseline_results['cdk_evaluation']['summary']\ntrained_cdk = trained_results['cdk_evaluation']['summary']\n\nprint(\"CDK-Specific Code Generation Comparison\")\nprint(\"=\" * 60)\nprint(f\"\\nSyntax Validity:\")\nprint(f\"  M0 (Baseline): {baseline_cdk['syntax_validity_percent']:.1f}%\")\nprint(f\"  M1 (Trained):  {trained_cdk['syntax_validity_percent']:.1f}%\")\nprint(f\"  Improvement:   {trained_cdk['syntax_validity_percent'] - baseline_cdk['syntax_validity_percent']:+.1f}%\")\n\nprint(f\"\\nKeyword Match Rate:\")\nprint(f\"  M0 (Baseline): {baseline_cdk.get('keyword_match_rate', 0)*100:.1f}%\")\nprint(f\"  M1 (Trained):  {trained_cdk.get('keyword_match_rate', 0)*100:.1f}%\")\nprint(f\"  Improvement:   {(trained_cdk.get('keyword_match_rate', 0) - baseline_cdk.get('keyword_match_rate', 0))*100:+.1f}%\")\n\nprint(f\"\\nMean Latency:\")\nprint(f\"  M0 (Baseline): {baseline_cdk['mean_latency_ms']:.1f}ms\")\nprint(f\"  M1 (Trained):  {trained_cdk['mean_latency_ms']:.1f}ms\")\nprint(f\"  Change:        {trained_cdk['mean_latency_ms'] - baseline_cdk['mean_latency_ms']:+.1f}ms\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Benchmarking Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if 'latency_benchmark' in baseline_results:\n    baseline_lat = baseline_results['latency_benchmark']\n    \n    print(\"Latency Benchmark Comparison\")\n    print(\"=\" * 60)\n    print(f\"\\nM0 (Baseline):\")\n    print(f\"  Mean:   {baseline_lat['mean_ms']:.1f}ms\")\n    print(f\"  Median: {baseline_lat['median_ms']:.1f}ms\")\n    print(f\"  P95:    {baseline_lat['p95_ms']:.1f}ms\")\n    print(f\"  P50 ≤ 300ms: {'✓' if baseline_lat.get('p50_requirement_met', False) else '✗'}\")\n    print(f\"  P95 ≤ 800ms: {'✓' if baseline_lat.get('p95_requirement_met', False) else '✗'}\")\nelse:\n    print(\"Latency benchmark data not available for baseline\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create comparison dataframe\ncomparison_data = {\n    'Metric': [\n        'Generic Syntax Validity (%)',\n        'Generic Keyword Match (%)',\n        'Generic Mean Latency (ms)',\n        'CDK Syntax Validity (%)',\n        'CDK Keyword Match (%)',\n        'CDK Mean Latency (ms)'\n    ],\n    'M0 (Baseline)': [\n        f\"{baseline_gen['syntax_validity_percent']:.1f}\",\n        f\"{baseline_gen.get('keyword_match_rate', 0)*100:.1f}\",\n        f\"{baseline_gen['mean_latency_ms']:.1f}\",\n        f\"{baseline_cdk['syntax_validity_percent']:.1f}\",\n        f\"{baseline_cdk.get('keyword_match_rate', 0)*100:.1f}\",\n        f\"{baseline_cdk['mean_latency_ms']:.1f}\"\n    ],\n    'M1 (Trained)': [\n        f\"{trained_gen['syntax_validity_percent']:.1f}\",\n        f\"{trained_gen.get('keyword_match_rate', 0)*100:.1f}\",\n        f\"{trained_gen['mean_latency_ms']:.1f}\",\n        f\"{trained_cdk['syntax_validity_percent']:.1f}\",\n        f\"{trained_cdk.get('keyword_match_rate', 0)*100:.1f}\",\n        f\"{trained_cdk['mean_latency_ms']:.1f}\"\n    ],\n    'Improvement': [\n        f\"{trained_gen['syntax_validity_percent'] - baseline_gen['syntax_validity_percent']:+.1f}\",\n        f\"{(trained_gen.get('keyword_match_rate', 0) - baseline_gen.get('keyword_match_rate', 0))*100:+.1f}\",\n        f\"{trained_gen['mean_latency_ms'] - baseline_gen['mean_latency_ms']:+.1f}\",\n        f\"{trained_cdk['syntax_validity_percent'] - baseline_cdk['syntax_validity_percent']:+.1f}\",\n        f\"{(trained_cdk.get('keyword_match_rate', 0) - baseline_cdk.get('keyword_match_rate', 0))*100:+.1f}\",\n        f\"{trained_cdk['mean_latency_ms'] - baseline_cdk['mean_latency_ms']:+.1f}\"\n    ]\n}\n\ndf_comparison = pd.DataFrame(comparison_data)\nprint(\"\\nModel Comparison Summary\")\nprint(\"=\" * 80)\nprint(df_comparison.to_string(index=False))\nprint(\"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create comprehensive comparison visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. Syntax Validity Comparison\nax1 = axes[0, 0]\ncategories = ['Generic', 'CDK']\nbaseline_validity = [\n    baseline_gen['syntax_validity_percent'],\n    baseline_cdk['syntax_validity_percent']\n]\ntrained_validity = [\n    trained_gen['syntax_validity_percent'],\n    trained_cdk['syntax_validity_percent']\n]\n\nx = np.arange(len(categories))\nwidth = 0.35\nax1.bar(x - width/2, baseline_validity, width, label='M0 (Baseline)', alpha=0.8)\nax1.bar(x + width/2, trained_validity, width, label='M1 (Trained)', alpha=0.8)\nax1.set_ylabel('Syntax Validity (%)', fontsize=11)\nax1.set_title('Syntax Validity Comparison', fontsize=12, fontweight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(categories)\nax1.legend()\nax1.grid(True, alpha=0.3, axis='y')\nax1.set_ylim([0, 105])\n\n# Add value labels\nfor i, (b, t) in enumerate(zip(baseline_validity, trained_validity)):\n    ax1.text(i - width/2, b + 2, f'{b:.1f}%', ha='center', va='bottom', fontsize=9)\n    ax1.text(i + width/2, t + 2, f'{t:.1f}%', ha='center', va='bottom', fontsize=9)\n\n# 2. Keyword Match Rate Comparison\nax2 = axes[0, 1]\nbaseline_keywords = [\n    baseline_gen.get('keyword_match_rate', 0) * 100,\n    baseline_cdk.get('keyword_match_rate', 0) * 100\n]\ntrained_keywords = [\n    trained_gen.get('keyword_match_rate', 0) * 100,\n    trained_cdk.get('keyword_match_rate', 0) * 100\n]\n\nax2.bar(x - width/2, baseline_keywords, width, label='M0 (Baseline)', alpha=0.8)\nax2.bar(x + width/2, trained_keywords, width, label='M1 (Trained)', alpha=0.8)\nax2.set_ylabel('Keyword Match Rate (%)', fontsize=11)\nax2.set_title('Keyword Match Rate Comparison', fontsize=12, fontweight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels(categories)\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\nax2.set_ylim([0, 105])\n\nfor i, (b, t) in enumerate(zip(baseline_keywords, trained_keywords)):\n    ax2.text(i - width/2, b + 2, f'{b:.1f}%', ha='center', va='bottom', fontsize=9)\n    ax2.text(i + width/2, t + 2, f'{t:.1f}%', ha='center', va='bottom', fontsize=9)\n\n# 3. Latency Comparison\nax3 = axes[1, 0]\nbaseline_latency = [\n    baseline_gen['mean_latency_ms'],\n    baseline_cdk['mean_latency_ms']\n]\ntrained_latency = [\n    trained_gen['mean_latency_ms'],\n    trained_cdk['mean_latency_ms']\n]\n\nax3.bar(x - width/2, baseline_latency, width, label='M0 (Baseline)', alpha=0.8)\nax3.bar(x + width/2, trained_latency, width, label='M1 (Trained)', alpha=0.8)\nax3.set_ylabel('Mean Latency (ms)', fontsize=11)\nax3.set_title('Inference Latency Comparison', fontsize=12, fontweight='bold')\nax3.set_xticks(x)\nax3.set_xticklabels(categories)\nax3.legend()\nax3.grid(True, alpha=0.3, axis='y')\n\nfor i, (b, t) in enumerate(zip(baseline_latency, trained_latency)):\n    ax3.text(i - width/2, b + 50, f'{b:.0f}ms', ha='center', va='bottom', fontsize=9)\n    ax3.text(i + width/2, t + 50, f'{t:.0f}ms', ha='center', va='bottom', fontsize=9)\n\n# 4. Overall Improvement\nax4 = axes[1, 1]\nimprovements = [\n    trained_gen['syntax_validity_percent'] - baseline_gen['syntax_validity_percent'],\n    trained_cdk['syntax_validity_percent'] - baseline_cdk['syntax_validity_percent'],\n    (trained_gen.get('keyword_match_rate', 0) - baseline_gen.get('keyword_match_rate', 0)) * 100,\n    (trained_cdk.get('keyword_match_rate', 0) - baseline_cdk.get('keyword_match_rate', 0)) * 100\n]\nlabels = ['Gen\\nSyntax', 'CDK\\nSyntax', 'Gen\\nKeywords', 'CDK\\nKeywords']\ncolors = ['green' if imp > 0 else 'red' for imp in improvements]\n\nbars = ax4.bar(range(len(improvements)), improvements, color=colors, alpha=0.7)\nax4.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\nax4.set_ylabel('Improvement (%)', fontsize=11)\nax4.set_title('M1 Improvement over M0', fontsize=12, fontweight='bold')\nax4.set_xticks(range(len(labels)))\nax4.set_xticklabels(labels)\nax4.grid(True, alpha=0.3, axis='y')\n\nfor i, (bar, imp) in enumerate(zip(bars, improvements)):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height + (1 if height > 0 else -1),\n             f'{imp:+.1f}%', ha='center', va='bottom' if height > 0 else 'top', fontsize=9)\n\nplt.tight_layout()\nplt.savefig(EVAL_DIR / 'model_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n✓ Visualization saved to: {EVAL_DIR / 'model_comparison.png'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Sample Comparison\n\nCompare specific generated code samples side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show side-by-side comparison of generated code\nprint(\"Detailed Sample Comparison\")\nprint(\"=\" * 80)\n\n# Get detailed results\nbaseline_gen_details = baseline_results['generic_evaluation']['detailed_results']\ntrained_gen_details = trained_results['generic_evaluation']['detailed_results']\n\n# Compare first few samples\nfor i in range(min(3, len(baseline_gen_details))):\n    baseline_sample = baseline_gen_details[i]\n    trained_sample = trained_gen_details[i]\n    \n    print(f\"\\nSample {i+1}: {baseline_sample['name']}\")\n    print(\"-\" * 80)\n    print(f\"Prompt: {baseline_sample['prompt'][:100]}...\")\n    \n    print(f\"\\nM0 (Baseline):\")\n    print(f\"  Generated: {baseline_sample['generated'][:150]}...\")\n    print(f\"  Syntax valid: {'✓' if baseline_sample['syntax_valid'] else '✗'}\")\n    print(f\"  Keywords: {len(baseline_sample['keywords_found'])}/{len(baseline_sample['keywords_expected'])}\")\n    print(f\"  Latency: {baseline_sample['latency_ms']:.1f}ms\")\n    \n    print(f\"\\nM1 (Trained):\")\n    print(f\"  Generated: {trained_sample['generated'][:150]}...\")\n    print(f\"  Syntax valid: {'✓' if trained_sample['syntax_valid'] else '✗'}\")\n    print(f\"  Keywords: {len(trained_sample['keywords_found'])}/{len(trained_sample['keywords_expected'])}\")\n    print(f\"  Latency: {trained_sample['latency_ms']:.1f}ms\")\n    \n    print(\"\\n\" + \"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create comprehensive comparison report\nreport = {\n    \"comparison_date\": \"2026-01-18\",\n    \"models_compared\": {\n        \"M0\": {\n            \"name\": baseline_results['model_name'],\n            \"type\": baseline_results['model_type'],\n            \"description\": \"Base model without fine-tuning\"\n        },\n        \"M1\": {\n            \"name\": trained_results['model_name'],\n            \"type\": trained_results['model_type'],\n            \"description\": \"Iteratively trained model\",\n            \"training_config\": trained_results.get('training_config', {})\n        }\n    },\n    \"generic_evaluation\": {\n        \"M0\": baseline_gen,\n        \"M1\": trained_gen,\n        \"improvement\": {\n            \"syntax_validity_percent\": trained_gen['syntax_validity_percent'] - baseline_gen['syntax_validity_percent'],\n            \"keyword_match_rate\": (trained_gen.get('keyword_match_rate', 0) - baseline_gen.get('keyword_match_rate', 0)) * 100,\n            \"mean_latency_ms\": trained_gen['mean_latency_ms'] - baseline_gen['mean_latency_ms']\n        }\n    },\n    \"cdk_evaluation\": {\n        \"M0\": baseline_cdk,\n        \"M1\": trained_cdk,\n        \"improvement\": {\n            \"syntax_validity_percent\": trained_cdk['syntax_validity_percent'] - baseline_cdk['syntax_validity_percent'],\n            \"keyword_match_rate\": (trained_cdk.get('keyword_match_rate', 0) - baseline_cdk.get('keyword_match_rate', 0)) * 100,\n            \"mean_latency_ms\": trained_cdk['mean_latency_ms'] - baseline_cdk['mean_latency_ms']\n        }\n    },\n    \"key_findings\": [\n        f\"M1 shows {trained_gen['syntax_validity_percent'] - baseline_gen['syntax_validity_percent']:+.1f}% improvement in generic syntax validity\",\n        f\"M1 shows {trained_cdk['syntax_validity_percent'] - baseline_cdk['syntax_validity_percent']:+.1f}% improvement in CDK syntax validity\",\n        f\"M1 keyword matching improved by {(trained_gen.get('keyword_match_rate', 0) - baseline_gen.get('keyword_match_rate', 0))*100:+.1f}% for generic tasks\",\n        f\"M1 keyword matching improved by {(trained_cdk.get('keyword_match_rate', 0) - baseline_cdk.get('keyword_match_rate', 0))*100:+.1f}% for CDK tasks\",\n        f\"Latency impact: {trained_gen['mean_latency_ms'] - baseline_gen['mean_latency_ms']:+.1f}ms for generic tasks\"\n    ],\n    \"recommendations\": [\n        \"Use M1 (trained model) for production deployment\" if trained_gen['syntax_validity_percent'] > baseline_gen['syntax_validity_percent'] else \"Consider additional training\",\n        \"Monitor latency in production environment\",\n        \"Continue iterative training with more diverse samples\",\n        \"Evaluate on additional CDK-specific test cases\"\n    ]\n}\n\n# Save report\nwith open(OUTPUT_FILE, 'w') as f:\n    json.dump(report, f, indent=2)\n\nprint(f\"✓ Comprehensive report saved to: {OUTPUT_FILE}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 80)\nprint(\"MODEL COMPARISON SUMMARY\")\nprint(\"=\" * 80)\n\nprint(\"\\nModels Evaluated:\")\nprint(f\"  M0 (Baseline): {baseline_results['model_name']}\")\nprint(f\"  M1 (Trained):  {trained_results['model_name']}\")\n\nprint(\"\\nKey Findings:\")\nfor i, finding in enumerate(report['key_findings'], 1):\n    print(f\"  {i}. {finding}\")\n\nprint(\"\\nRecommendations:\")\nfor i, rec in enumerate(report['recommendations'], 1):\n    print(f\"  {i}. {rec}\")\n\nprint(\"\\nOutputs Generated:\")\nprint(f\"  ✓ Comparison visualization: {EVAL_DIR / 'model_comparison.png'}\")\nprint(f\"  ✓ Detailed report: {OUTPUT_FILE}\")\n\nprint(\"=\" * 80)\nprint(\"\\nEvaluation complete! Review the visualizations and report for detailed insights.\")\nprint(\"\\nNext: Notebook 07 - Test inference server API\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
